{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "In this notebook, we train our delay prediction model. We use the real-world SBB data to train the delay prediction model. We use the follwing features of a trip to train the model:\n",
    "1. Stop id\n",
    "2. Product id (bus, tram, zug)\n",
    "3. Time of the day\n",
    "4. Day of the week\n",
    "5. Week of the year\n",
    "\n",
    "We tried two different models to predict the delays: 1. Linear regression model and 2. Logistic Regression model. \\\n",
    "We found that the logistic regression model performs better and decided to use this for our final algorithm. We put the delays into bins of <0 min, 0-1 min, 1-2 min and upto 12 min. Our trained model then predicts the probability of the given trip to have a delay lying the the corresponding bin. We use these probabilities in our overall algorithm to predict the probability of success of the overall trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'moiseev-final', 'executorMemory': '8G', 'executorCores': 4, 'numExecutors': 10, 'driverMemory': '8G', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>7137</td><td>application_1618324153128_6842</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6842/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster078.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6842_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7155</td><td>application_1618324153128_6860</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6860/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6860_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7156</td><td>application_1618324153128_6861</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6861/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6861_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7159</td><td>application_1618324153128_6864</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6864/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6864_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7161</td><td>application_1618324153128_6866</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6866/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster077.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6866_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7165</td><td>application_1618324153128_6871</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6871/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6871_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7166</td><td>application_1618324153128_6872</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6872/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster077.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6872_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7168</td><td>application_1618324153128_6874</td><td>pyspark</td><td>shutting_down</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6874/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6874_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "import os\n",
    "username = 'moiseev'\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell='{ \"name\":\"%s-final\", \"executorMemory\":\"8G\", \"executorCores\":4, \"numExecutors\":10, \"driverMemory\": \"8G\" }' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>7169</td><td>application_1618324153128_6875</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6875/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6875_01_000001/ebouille\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'username' as 'username' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- operator_abbr: string (nullable = true)\n",
      " |-- operator_name: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- line_id: string (nullable = true)\n",
      " |-- line_text: string (nullable = true)\n",
      " |-- circulation_id: string (nullable = true)\n",
      " |-- transportation_text: string (nullable = true)\n",
      " |-- is_extra: string (nullable = true)\n",
      " |-- is_cancelled: string (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- scheduled_arrival_time: string (nullable = true)\n",
      " |-- actual_arrival_time: string (nullable = true)\n",
      " |-- arrival_forecast_status: string (nullable = true)\n",
      " |-- scheduled_departure_time: string (nullable = true)\n",
      " |-- actual_departure_time: string (nullable = true)\n",
      " |-- departure_forecast_status: string (nullable = true)\n",
      " |-- is_transit: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Read SBB data\n",
    "\n",
    "sbb_connections = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "sbb_connections = sbb_connections.selectExpr(\n",
    "    \"betriebstag as date\",\n",
    "    \n",
    "    \"fahrt_bezeichner as trip_id\",\n",
    "    \n",
    "    \"betreiber_id as operator_id\",\n",
    "    \"betreiber_abk as operator_abbr\",\n",
    "    \"betreiber_name as operator_name\",\n",
    "    \n",
    "    \"produkt_id as product_id\",\n",
    "    \"linien_id as line_id\",\n",
    "    \"linien_text as line_text\",\n",
    "    \"umlauf_id as circulation_id\",\n",
    "    \"verkehrsmittel_text as transportation_text\",\n",
    "    \"zusatzfahrt_tf as is_extra\",\n",
    "    \"faellt_aus_tf as is_cancelled\",\n",
    "    \"haltestellen_name as stop_name\",\n",
    "    # The bpuic corresponds to the stop_id in the sbb_stops from the geostops file\n",
    "    \"bpuic as stop_id\",\n",
    "    \n",
    "    \"ankunftszeit as scheduled_arrival_time\",\n",
    "    \"an_prognose as actual_arrival_time\",\n",
    "    \"an_prognose_status as arrival_forecast_status\", \n",
    "    \n",
    "    \"abfahrtszeit as scheduled_departure_time\",\n",
    "    \"ab_prognose as actual_departure_time\",\n",
    "    \"ab_prognose_status as departure_forecast_status\",\n",
    "    \n",
    "    \"durchfahrt_tf as is_transit\"\n",
    ")\n",
    "sbb_connections.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read other relevant data\n",
    "stop_map_df = spark.read.parquet(\"/user/{}/stop_map.parquet\".format(username))\n",
    "trip_map_df = spark.read.parquet(\"/user/{}/trip_map.parquet\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- operator_id: string (nullable = true)\n",
      " |-- operator_abbr: string (nullable = true)\n",
      " |-- operator_name: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- line_id: string (nullable = true)\n",
      " |-- line_text: string (nullable = true)\n",
      " |-- circulation_id: string (nullable = true)\n",
      " |-- transportation_text: string (nullable = true)\n",
      " |-- is_extra: string (nullable = true)\n",
      " |-- is_cancelled: string (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- scheduled_arrival_time: string (nullable = true)\n",
      " |-- actual_arrival_time: string (nullable = true)\n",
      " |-- arrival_forecast_status: string (nullable = true)\n",
      " |-- scheduled_departure_time: string (nullable = true)\n",
      " |-- actual_departure_time: string (nullable = true)\n",
      " |-- departure_forecast_status: string (nullable = true)\n",
      " |-- is_transit: string (nullable = true)\n",
      " |-- stop_id: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "def rename(df, rename_df):\n",
    "    column = rename_df.columns[0]\n",
    "    return df.join(rename_df, column, \"inner\")\\\n",
    "        .drop(column)\\\n",
    "        .withColumnRenamed(\"new_id\", column)\n",
    "\n",
    "# Training only on fraction of data due to time limitations and Spark cluster instability.\n",
    "# In our experience, obtained model is still good enough.\n",
    "conns = rename(sbb_connections, stop_map_df).sample(False, 0.01)\n",
    "conns = conns.withColumn(\"product_id\", F.lower(F.col(\"product_id\")))\n",
    "conns.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify distinct product ids present in data\n",
    "product_ids = conns.select(F.col(\"product_id\")).distinct().toPandas()[\"product_id\"].tolist()\n",
    "product_id_map = {\n",
    "    u'': 0,\n",
    "    u'bus': 1,\n",
    "    u'tram': 2,\n",
    "    u'zug': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement one-hot encoding for product_id and stop_id\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StandardScaler\n",
    "conns = conns.withColumn(\"product_id_int\", F.udf(lambda x: product_id_map[x], LongType())(F.col(\"product_id\")))\n",
    "ohe = OneHotEncoder(inputCol=\"product_id_int\", outputCol=\"product_id_oh\", dropLast=False)\n",
    "conns = ohe.transform(conns)\n",
    "ohe = OneHotEncoder(inputCol=\"stop_id\", outputCol=\"stop_id_oh\", dropLast=False)\n",
    "conns = ohe.transform(conns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352.0"
     ]
    }
   ],
   "source": [
    "# Some helper functions\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DoubleType, BooleanType\n",
    "\n",
    "def day_of_week(x):\n",
    "    date_time = datetime.strptime(x, \"%d.%m.%Y\")\n",
    "    return float(date_time.weekday())\n",
    "\n",
    "def week_of_year(x):\n",
    "    date_time = datetime.strptime(x, \"%d.%m.%Y\")\n",
    "    return float(date_time.isocalendar()[1])\n",
    "\n",
    "def to_minutes(x):\n",
    "    try:\n",
    "        date_time = datetime.strptime(x, \"%d.%m.%Y %H:%M\")\n",
    "        a_timedelta = date_time - datetime(1900, 1, 1)\n",
    "    except:\n",
    "        try:\n",
    "            date_time = datetime.strptime(x, \"%d.%m.%Y %H:%M:%S\")\n",
    "            a_timedelta = date_time - datetime(1900, 1, 1)\n",
    "        except:\n",
    "            raise ValueError(x)\n",
    "        \n",
    "    return a_timedelta.total_seconds() / 60\n",
    "\n",
    "def minute_of_day(minutes):\n",
    "    return minutes % (24 * 60)\n",
    "\n",
    "print(to_minutes(\"02.02.2020 05:52\") % (24 * 60))\n",
    "\n",
    "to_minutes_udf = F.udf(to_minutes, DoubleType())\n",
    "minute_of_day_udf = F.udf(minute_of_day, DoubleType())\n",
    "week_of_year_udf = F.udf(week_of_year, DoubleType())\n",
    "day_of_week_udf = F.udf(day_of_week, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove unwanted rows (those having empty date, arrival/departure times)\n",
    "# Introduce columns for week of the year, day of the week, time fo the day\n",
    "conns = conns\\\n",
    "    .filter(\"date !=''\")\\\n",
    "    .filter(\"scheduled_arrival_time != ''\")\\\n",
    "    .filter(\"actual_arrival_time != ''\")\\\n",
    "    .withColumn(\"scheduled_arrival_minutes\", to_minutes_udf(F.col(\"scheduled_arrival_time\")))\\\n",
    "    .withColumn(\"actual_arrival_minutes\", to_minutes_udf(F.col(\"actual_arrival_time\")))\\\n",
    "    .withColumn(\"week_of_year\", week_of_year_udf(F.col(\"date\")))\\\n",
    "    .withColumn(\"day_of_week\", day_of_week_udf(F.col(\"date\")))\n",
    "\n",
    "conns = conns.withColumn(\"delay\", F.col(\"actual_arrival_minutes\") - F.col(\"scheduled_arrival_minutes\"))\\\n",
    "    .withColumn(\"minute_of_day\", minute_of_day_udf(F.col(\"scheduled_arrival_minutes\")))\\\n",
    "    .withColumn(\"square_from_midday\", (24 * 60 / 2 - F.col(\"minute_of_day\")) ** 2)\\\n",
    "    .withColumn(\"abs_from_midday\", F.col(\"square_from_midday\") ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assemble the features defined above\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['minute_of_day', 'square_from_midday', 'abs_from_midday', 'product_id_oh', 'stop_id_oh', 'week_of_year', 'day_of_week'],\n",
    "    outputCol='features')\n",
    "\n",
    "data = assembler.transform(conns).select('features', 'delay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scale the features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(data)\n",
    "data = scaler_model.transform(data).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model features to HDFS\n",
    "username = 'moiseev'\n",
    "scaler.write().overwrite().save('/user/{}/delays_scaler'.format(username))\n",
    "scaler_model.write().overwrite().save('/user/{}/delays_scaler_model'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='scaled_features', labelCol='delay')\n",
    "model = lr.fit(data.drop(\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_path = '/user/{}/delays_lr'.format(username)\n",
    "model_path = '/user/{}/delays_lr_model'.format(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr.write().overwrite().save(lr_path)\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform delay values to bins\n",
    "def to_label(x):\n",
    "    bounds = list(range(12))\n",
    "    for b in bounds[:-1]:\n",
    "        if x <= b:\n",
    "            return b\n",
    "    return bounds[-1]\n",
    "        \n",
    "to_label_udf = F.udf(to_label, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_data = data.withColumn(\"delay_bin\", to_label_udf(F.col('delay'))).drop(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "log = LogisticRegression(featuresCol='scaled_features', labelCol='delay_bin')\n",
    "log_model = log.fit(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_path = '/user/{}/delays_log'.format(username)\n",
    "log_model_path = '/user/{}/delays_log_model'.format(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log.write().overwrite().save(log_path)\n",
    "log_model.write().overwrite().save(log_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_data_pred = log_model.transform(log_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
