{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "\n",
    "In this notebook, we implement the validation part of our project. \\\n",
    "To validate a trip predicted by the route planning algorithm, we take as input the list of nodes (stops) and edges (connecting trips), and then we compare the same trips in the SBB real-world data. We see if the predicted route would lead to one of the following real-world events:\n",
    "1. SUCCESSFUL TRIP\n",
    "2. MISSED CONNECTION\n",
    "3. Connection not found (this occurs due to a slight mismatch between the timetable data used for route prediction and the SBB data) \n",
    "We take consider 50 random pairs of origin and destination stations and use 3 different algorithms to find the route. For each of these we then find the percentage of successful trip as $\\frac{\\text{#SUCCESSFUL TRIPS}}{\\text{#SUCCESSFUL TRIPS+#MISSED CONNECTIONS}}$. (We ignore the 'Connection not found' cases). \\\n",
    "We find the following results:\n",
    "| Algorithm                                           | Percentage of successful trips|\n",
    "|:---------------------------------------------------:|:-----------------------------:|\n",
    "| Shortest path algorithm (no delay considerations)   | 72.4%                         |\n",
    "| Our algorithm (confidence 0.8)                      | 90.3%                         |   \n",
    "| Our algorithm (confidence 0.96)                     | 96.7%                         |   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'moiseev-final', 'executorMemory': '4G', 'executorCores': 4, 'numExecutors': 10, 'driverMemory': '32G', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>7192</td><td>application_1618324153128_6907</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6907/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6907_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7196</td><td>application_1618324153128_6922</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6922/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6922_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7197</td><td>application_1618324153128_6924</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6924/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6924_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7198</td><td>application_1618324153128_6927</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6927/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6927_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7199</td><td>application_1618324153128_6928</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6928/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster078.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6928_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7201</td><td>application_1618324153128_6930</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6930/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6930_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7203</td><td>application_1618324153128_6932</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6932/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6932_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7204</td><td>application_1618324153128_6933</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6933/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster077.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6933_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr><tr><td>7205</td><td>application_1618324153128_6934</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6934/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6934_01_000001/ebouille\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "import os\n",
    "username = 'moiseev'\n",
    "username = os.environ['JUPYTERHUB_USER'] # Uncomment if you want to use your models and data.\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell='{ \"name\":\"%s-final\", \"executorMemory\":\"4G\", \"executorCores\":4, \"numExecutors\":10, \"driverMemory\": \"32G\" }' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>7206</td><td>application_1618324153128_6935</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster040.iccluster.epfl.ch:8088/proxy/application_1618324153128_6935/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e01_1618324153128_6935_01_000001/ebouille\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'username' as 'username' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i username -t str -n username "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some helper functions\n",
    "\n",
    "# To correctly format the time\n",
    "def format_time(time):\n",
    "    return \"{0}:{1}\".format(str(time).split(\":\")[0].zfill(2), str(time).split(\":\")[1].zfill(2))#.zfill(2)\n",
    "\n",
    "# To add minutes to a given time\n",
    "def add_minutes(t, minutes):\n",
    "    (hour_t, min_t) = tuple(map(int, t.split(':')))\n",
    "    new_min = min_t + minutes\n",
    "    if new_min > 59:\n",
    "        new_min = new_min % 60\n",
    "        hour_t = hour_t + 1\n",
    "    ret = str(hour_t)+':'+str(new_min)\n",
    "    return ret\n",
    "\n",
    "# To subtract minutes from a give time\n",
    "def sub_minutes(t, minutes):\n",
    "    (hour_t, min_t) = tuple(map(int, t.split(':')))\n",
    "    new_min = min_t - minutes\n",
    "    if new_min < 0:\n",
    "        new_min = new_min % 60\n",
    "        hour_t = hour_t - 1\n",
    "    ret = str(hour_t)+':'+str(new_min)\n",
    "    return ret\n",
    "\n",
    "# Create a 5-min time bracket\n",
    "def create_time_bracket(t):\n",
    "    return [format_time(sub_minutes(t,2)), format_time(sub_minutes(t,1)), format_time(t), format_time(add_minutes(t,1)), format_time(add_minutes(t,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to read relevant data for validation from hdfs\n",
    "def read_data():\n",
    "    sbb_connections = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "    sbb_connections = sbb_connections.selectExpr(\n",
    "        \"betriebstag as date\",\n",
    "\n",
    "        \"fahrt_bezeichner as trip_id\",\n",
    "\n",
    "        \"betreiber_id as operator_id\",\n",
    "        \"betreiber_abk as operator_abbr\",\n",
    "        \"betreiber_name as operator_name\",\n",
    "\n",
    "        \"produkt_id as product_id\",\n",
    "        \"linien_id as line_id\",\n",
    "        \"linien_text as line_text\",\n",
    "        \"umlauf_id as circulation_id\",\n",
    "        \"verkehrsmittel_text as transportation_text\",\n",
    "        \"zusatzfahrt_tf as is_extra\",\n",
    "        \"faellt_aus_tf as is_cancelled\",\n",
    "        \"haltestellen_name as stop_name\",\n",
    "        # The bpuic corresponds to the stop_id in the sbb_stops from the geostops file\n",
    "        \"bpuic as stop_id\",\n",
    "\n",
    "        \"ankunftszeit as scheduled_arrival_time\",\n",
    "        \"an_prognose as actual_arrival_time\",\n",
    "        \"an_prognose_status as arrival_forecast_status\", \n",
    "\n",
    "        \"abfahrtszeit as scheduled_departure_time\",\n",
    "        \"ab_prognose as actual_departure_time\",\n",
    "        \"ab_prognose_status as departure_forecast_status\",\n",
    "\n",
    "        \"durchfahrt_tf as is_transit\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    stops = spark.read.parquet(\"/user/{}/stops.parquet\".format(username))\n",
    "    routes = spark.read.parquet(\"/user/{}/routes.parquet\".format(username))\n",
    "    trips = spark.read.parquet(\"/user/{}/trips.parquet\".format(username))\n",
    "    return sbb_connections, stops, routes, trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to validate the given trip\n",
    "# Return values:\n",
    "# 0: TRIP SUCCESSFUL\n",
    "# 1: CONNECTION MISSED\n",
    "# 2: Couldn't find connection\n",
    "\n",
    "def validate_trip(nodes, edges, conns, stops, routes, trips, date_of_journey):\n",
    "\n",
    "    flag = 0\n",
    "    \n",
    "    # Filter stops to retain only relevant stops in journey\n",
    "    stops_filt = stops\\\n",
    "        .select(\"stop_id\", \"stop_name\")\\\n",
    "        .filter(F.col(\"stop_id\").isin(nodes))\\\n",
    "        .toPandas()\\\n",
    "        .set_index(\"stop_id\")\n",
    "    \n",
    "    trip_ids = set()\n",
    "    for e in edges:\n",
    "        if e['trip_id'] != 'walk':\n",
    "            trip_ids.add(e['trip_id'])\n",
    "    trip_ids = list(trip_ids)\n",
    "    \n",
    "    # Obtain the routes and operators of the trips which comprose the journey\n",
    "    trip_to_route = routes\\\n",
    "        .join(trips, 'route_id', 'left')\\\n",
    "        .filter(F.col(\"trip_id\").isin(trip_ids))\\\n",
    "        .select(\"route_desc\", \"route_short_name\", \"trip_id\", \"agency_id\")\\\n",
    "        .toPandas().set_index('trip_id')\n",
    "    \n",
    "    # Filter SBB data to contain inly interested stops and on date of journey\n",
    "    # Add columns for arrival/departure time, both scheduled and actual\n",
    "    conns_filt = conns\\\n",
    "                .filter(F.col(\"stop_name\").isin(list(stops_filt['stop_name'])))\\\n",
    "                .filter(F.col(\"date\")==date_of_journey)\\\n",
    "                .withColumn(\"scheduled_arrival_time_ft\", F.substring(F.col(\"scheduled_arrival_time\"),12,5))\\\n",
    "                .withColumn(\"scheduled_departure_time_ft\", F.substring(F.col(\"scheduled_departure_time\"),12,5))\\\n",
    "                .withColumn(\"actual_arrival_time_ft\", F.substring(F.col(\"actual_arrival_time\"),12,8))\\\n",
    "                .withColumn(\"actual_departure_time_ft\", F.substring(F.col(\"actual_departure_time\"),12,8))\\\n",
    "                .withColumn(\"operator_id_ft\", F.substring(F.col(\"operator_id\"),4,12))\\\n",
    "                .select(\"operator_id_ft\",\"stop_name\",\"scheduled_arrival_time_ft\", \"scheduled_departure_time_ft\", \"actual_arrival_time_ft\", \"actual_departure_time_ft\", \"stop_id\", \"product_id\", \"line_text\", \"circulation_id\")\\\n",
    "                .toPandas()\n",
    "    \n",
    "    curr_trip_id = edges[0]['trip_id']\n",
    "    if curr_trip_id=='walk':\n",
    "        walking_time = edges[0]['walking_time']\n",
    "        scheduled_walking_start_time = pd.to_datetime(format_time(edges[0]['dep']), infer_datetime_format=True)\n",
    "        actual_walking_start_time = pd.to_datetime(format_time(edges[0]['dep']), infer_datetime_format=True)\n",
    "    else:\n",
    "        walking_time = 0\n",
    "\n",
    "    for e in range(len(edges)):\n",
    "        stop_id = nodes[e]\n",
    "        # If trip_id changes, ie a transfer occurs on the journey\n",
    "        if(edges[e]['trip_id']!=curr_trip_id):\n",
    "            if curr_trip_id=='walk':\n",
    "                scheduled_arrival = scheduled_walking_start_time + pd.Timedelta(minutes=math.ceil(walking_time))\n",
    "                actual_arrival = actual_walking_start_time + pd.Timedelta(minutes=math.ceil(walking_time))\n",
    "            else:\n",
    "                # Obtain the corresponding trip from SBB data which arrives at the desired stop \n",
    "                # using the same mode of transport, and in a 5-min time bracket\n",
    "                arrival_edge = conns_filt[\\\n",
    "                                         (conns_filt['stop_name'].isin([stops_filt.loc[stop_id]['stop_name']])) &\\\n",
    "                                         (conns_filt['operator_id_ft']==str(trip_to_route.loc[curr_trip_id]['agency_id'])) &\\\n",
    "                                         (conns_filt['line_text'].str.contains(trip_to_route.loc[curr_trip_id]['route_short_name'])) &\\\n",
    "                                         (conns_filt['scheduled_arrival_time_ft'].isin(create_time_bracket(edges[e-1]['arr'])))\\\n",
    "                                        ]\n",
    "                if (len(arrival_edge)==0):\n",
    "                    print(\"Couldn't find connection!\")\n",
    "                    flag = 2\n",
    "                    break\n",
    "                scheduled_arrival = min(pd.to_datetime(arrival_edge['scheduled_arrival_time_ft'], infer_datetime_format=True))\n",
    "                actual_arrival = min(pd.to_datetime(arrival_edge['actual_arrival_time_ft'], infer_datetime_format=True))\n",
    "\n",
    "            if edges[e]['trip_id']=='walk':\n",
    "                scheduled_departure = scheduled_arrival + pd.Timedelta(minutes=2)\n",
    "                actual_departure = actual_arrival + pd.Timedelta(minutes=2)\n",
    "                scheduled_walking_start_time = scheduled_arrival\n",
    "                actual_walking_start_time = actual_arrival\n",
    "            else:\n",
    "                # Obtain the corresponding trip from SBB data which departs from the desired stop \n",
    "                # using the same mode of transport, and in a 5-min time bracket\n",
    "                dep_edge = conns_filt[\\\n",
    "                                         (conns_filt['stop_name'].isin([stops_filt.loc[stop_id]['stop_name']])) &\\\n",
    "                                         (conns_filt['operator_id_ft']==str(trip_to_route.loc[edges[e]['trip_id']]['agency_id'])) &\\\n",
    "                                         (conns_filt['line_text'].str.contains(trip_to_route.loc[edges[e]['trip_id']]['route_short_name'])) &\\\n",
    "                                         (conns_filt['scheduled_departure_time_ft'].isin(create_time_bracket(edges[e]['dep'])))\\\n",
    "                                        ]\n",
    "                if (len(dep_edge)==0):\n",
    "                    print(\"Couldn't find connection!\")\n",
    "                    flag = 2\n",
    "                    break\n",
    "                scheduled_departure = max(pd.to_datetime(dep_edge['scheduled_departure_time_ft'], infer_datetime_format=True))\n",
    "                actual_departure = max(pd.to_datetime(dep_edge['actual_departure_time_ft'], infer_datetime_format=True))\n",
    "\n",
    "            if curr_trip_id!='walk':\n",
    "                # If the actual departure from the stop is within 2 minutes of the actual\n",
    "                # arrivak time, the connection was missed!\n",
    "                if (actual_arrival+pd.Timedelta(minutes=2)>actual_departure):\n",
    "                    print(\"Connection MISSED!\")\n",
    "                    flag = 1\n",
    "                    break\n",
    "            else:\n",
    "                if (actual_arrival>actual_departure):\n",
    "                    print(\"Connection MISSED!\")\n",
    "                    flag = 1\n",
    "                    break\n",
    "            curr_trip_id = edges[e]['trip_id']\n",
    "    if flag==0:\n",
    "        print(\"Trip SUCCESSFUL!\")\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read relevant data\n",
    "conns, stops, routes, trips = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Read saved trips from file \n",
    "\n",
    "# File A_08 containes route predictions using our algorithm with a confidence of 0.8\n",
    "with open('../data/A_08.pickle', 'rb') as config_dictionary_file:\n",
    "    ret = pickle.load(config_dictionary_file)\n",
    "    \n",
    "# File A_096 containes route predictions using our algorithm with a confidence of 0.96\n",
    "with open('../data/A_096.pickle', 'rb') as config_dictionary_file:\n",
    "    ret1 = pickle.load(config_dictionary_file)\n",
    "    \n",
    "# File A_base1 containes route predictions using the baseline shortest path algorithm, with \n",
    "# no delay considerations\n",
    "with open('../data/A_base1.pickle', 'rb') as config_dictionary_file:\n",
    "    ret_base = pickle.load(config_dictionary_file)\n",
    "\n",
    "nodes = [ret[i][0][0][0] for i in range(len(ret))]\n",
    "edges = [pd.DataFrame(ret[i][0][0][1]) for i in range(len(ret))]\n",
    "nodes1 = [ret1[i][0][0][0] for i in range(len(ret1))]\n",
    "edges1 = [pd.DataFrame(ret1[i][0][0][1]) for i in range(len(ret1))]\n",
    "nodes_b = [ret_base[i][0][0][0] for i in range(len(ret_base))]\n",
    "edges_b = [pd.DataFrame(ret_base[i][0][0][1]) for i in range(len(ret_base))]\n",
    "\n",
    "nodes = pd.DataFrame([nodes])\n",
    "edges = pd.DataFrame([edges])\n",
    "\n",
    "nodes1 = pd.DataFrame([nodes1])\n",
    "edges1 = pd.DataFrame([edges1])\n",
    "\n",
    "nodes_b = pd.DataFrame([nodes_b])\n",
    "edges_b = pd.DataFrame([edges_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Send the above read variable to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'nodes' as 'nodes' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i nodes -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'edges' as 'edges' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i edges -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'nodes_b' as 'nodes_b' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i nodes_b -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'edges_b' as 'edges_b' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i edges_b -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'nodes1' as 'nodes1' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i nodes1 -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'edges1' as 'edges1' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i edges1 -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find connection!\n",
      "Trip SUCCESSFUL!\n",
      "Connection MISSED!\n",
      "Trip SUCCESSFUL!\n",
      "Trip SUCCESSFUL!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Trip SUCCESSFUL!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Trip SUCCESSFUL!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Trip SUCCESSFUL!\n",
      "Trip SUCCESSFUL!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Connection MISSED!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Trip SUCCESSFUL!\n",
      "Trip SUCCESSFUL!\n",
      "Trip SUCCESSFUL!\n",
      "Couldn't find connection!\n",
      "Couldn't find connection!\n",
      "Trip SUCCESSFUL!"
     ]
    }
   ],
   "source": [
    "# Run validation on the predicted routes, using the 3 different algorithms. \n",
    "# We aim to find the fraction of trips which succeed\n",
    "# NOTE: We have saved 50 routes between randomly chosen origin and destination\n",
    "# stations, and validation of these 50 trips can take some time. To test for a \n",
    "# smaller set of trip, reduce the variable NUM_TRIPS in the line below.\n",
    "\n",
    "NUM_TRIPS = 10\n",
    "\n",
    "import ast\n",
    "count_base = 0\n",
    "success_base = 0\n",
    "count = 0\n",
    "success = 0\n",
    "count1 = 0\n",
    "success1 = 0\n",
    "for i in range(NUM_TRIPS):\n",
    "    nodes_i = nodes.toPandas()[str(i)].item()\n",
    "    edges_i = ast.literal_eval(edges.toPandas()['_corrupt_record'].item())[str(i)]\n",
    "    flag = validate_trip(nodes_i, edges_i, conns, stops, routes, trips, \"16.05.2019\")\n",
    "    if flag==0:\n",
    "        success+=1\n",
    "        count+=1\n",
    "    if flag==1:\n",
    "        count+=1\n",
    "    nodes_i = nodes1.toPandas()[str(i)].item()\n",
    "    edges_i = ast.literal_eval(edges1.toPandas()['_corrupt_record'].item())[str(i)]\n",
    "    flag = validate_trip(nodes_i, edges_i, conns, stops, routes, trips, \"16.05.2019\")\n",
    "    if flag==0:\n",
    "        success1+=1\n",
    "        count1+=1\n",
    "    if flag==1:\n",
    "        count1+=1\n",
    "    nodes_i = nodes_b.toPandas()[str(i)].item()\n",
    "    edges_i = ast.literal_eval(edges_b.toPandas()['_corrupt_record'].item())[str(i)]\n",
    "    flag = validate_trip(nodes_i, edges_i, conns, stops, routes, trips, \"16.05.2019\")\n",
    "    if flag==0:\n",
    "        success_base+=1\n",
    "        count_base+=1\n",
    "    if flag==1:\n",
    "        count_base+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fraction of successful trips using shortest path algo: ', 0.6)\n",
      "('Fraction of successful trips using our algo, confidence = 0.80: ', 1.0)\n",
      "('Fraction of successful trips using our algo, confidence = 0.96: ', 1.0)"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of successful trips using shortest path algo: \", success_base*1.0/count_base)\n",
    "print(\"Fraction of successful trips using our algo, confidence = 0.80: \", success*1.0/count)\n",
    "print(\"Fraction of successful trips using our algo, confidence = 0.96: \", success1*1.0/count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
